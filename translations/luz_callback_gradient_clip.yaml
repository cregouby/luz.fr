title:
  original: Gradient clipping callback
  translation: ~
arguments:
  max_norm:
    original: '(float or int): max norm of the gradients'
    translation: ~
  norm_type:
    original: |-
      (float or int): type of the used p-norm. Can be \code{Inf} for
      infinity norm.
    translation: ~
description:
  original: |
    By adding the GradientClip callback, the gradient \code{norm_type} (default:2) norm
    is clipped to at most \code{max_norm} (default:1) using \code{\link[torch:nn_utils_clip_grad_norm_]{torch::nn_utils_clip_grad_norm_()}},
    which can avoid loss divergence.
  translation: ~
references:
  original: |
    See FastAI \href{https://docs.fast.ai/callback.training.html#GradientClip}{documentation}
    for the GradientClip callback.
  translation: ~
untranslatable:
- alias
- name
- keyword
- concept
- usage
