title:
  original: Learning rate scheduler callback
  translation: ~
arguments:
  lr_scheduler:
    original: |-
      A \code{\link[torch:lr_scheduler]{torch::lr_scheduler()}} that will be initialized with
      the optimizer and the \code{...} parameters.
    translation: ~
  '...':
    original: |-
      Additional arguments passed to \code{lr_scheduler} together with
      the optimizers.
    translation: ~
  call_on:
    original: |-
      The callback breakpoint that \code{scheduler$step()} is called.
      Default is \code{'on_epoch_end'}. See \code{\link[=luz_callback]{luz_callback()}} for more information.
    translation: ~
  opt_name:
    original: |-
      name of the optimizer that will be affected by this callback.
      Should match the name given in \code{set_optimizers}. If your module has a single
      optimizer, \code{opt_name} is not used.
    translation: ~
value:
  original: |
    A \code{\link[=luz_callback]{luz_callback()}} generator.
  translation: ~
description:
  original: |
    Initializes and runs \code{\link[torch:lr_scheduler]{torch::lr_scheduler()}}s.
  translation: ~
examples:
  original: |
    if (torch::torch_is_installed()) {
    cb <- luz_callback_lr_scheduler(torch::lr_step, step_size = 30)
    }
  translation: ~
seealso:
  original: "Other luz_callbacks: \n\\code{\\link{luz_callback_auto_resume}()},\n\\code{\\link{luz_callback_csv_logger}()},\n\\code{\\link{luz_callback_early_stopping}()},\n\\code{\\link{luz_callback_interrupt}()},\n\\code{\\link{luz_callback_keep_best_model}()},\n\\code{\\link{luz_callback_metrics}()},\n\\code{\\link{luz_callback_mixed_precision}()},\n\\code{\\link{luz_callback_mixup}()},\n\\code{\\link{luz_callback_model_checkpoint}()},\n\\code{\\link{luz_callback_profile}()},\n\\code{\\link{luz_callback_progress}()},\n\\code{\\link{luz_callback_resume_from_checkpoint}()},\n\\code{\\link{luz_callback_train_valid}()},\n\\code{\\link{luz_callback}()}\n"
  translation: ~
untranslatable:
- alias
- name
- keyword
- concept
- usage
