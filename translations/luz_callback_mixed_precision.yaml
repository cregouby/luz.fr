title:
  original: Automatic Mixed Precision callback
  translation: ~
arguments:
  '...':
    original: Passed to \code{\link[torch:cuda_amp_grad_scaler]{torch::cuda_amp_grad_scaler()}}.
    translation: ~
value:
  original: |
    A \code{luz_callback}
  translation: ~
description:
  original: |
    This callback will enable \code{\link[torch:local_autocast]{torch::local_autocast()}} training model forward
    and during loss computation. It will then disable autocast and scale the loss
    before \code{backward()} and \code{opt$step()}. See \href{https://torch.mlverse.org/docs/articles/amp.html}{here}
    for more information.
  translation: ~
seealso:
  original: "Other luz_callbacks: \n\\code{\\link{luz_callback_auto_resume}()},\n\\code{\\link{luz_callback_csv_logger}()},\n\\code{\\link{luz_callback_early_stopping}()},\n\\code{\\link{luz_callback_interrupt}()},\n\\code{\\link{luz_callback_keep_best_model}()},\n\\code{\\link{luz_callback_lr_scheduler}()},\n\\code{\\link{luz_callback_metrics}()},\n\\code{\\link{luz_callback_mixup}()},\n\\code{\\link{luz_callback_model_checkpoint}()},\n\\code{\\link{luz_callback_profile}()},\n\\code{\\link{luz_callback_progress}()},\n\\code{\\link{luz_callback_resume_from_checkpoint}()},\n\\code{\\link{luz_callback_train_valid}()},\n\\code{\\link{luz_callback}()}\n"
  translation: ~
untranslatable:
- alias
- name
- keyword
- concept
- usage
