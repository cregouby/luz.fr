title:
  original: Computes the multi-class AUROC
  translation: Calcule la surface sour la courbe de ROC dans le cas multi-classe.
arguments:
  num_thresholds:
    original: |-
      Number of thresholds used to compute confusion matrices.
      In that case, thresholds are created by getting \code{num_thresholds} values linearly
      spaced in the unit interval.
    translation: |-
      Nombre de seuils utilisés pour calculer les matrices de confusion.
      Lorsqu'utilisés, les seuils sont créés en prenant \code{num_thresholds} valeurs 
      linéairement espacées sur l'intervale [0, 1].
  thresholds:
    original: |-
      (optional) If threshold are passed, then those are used to compute the
      confusion matrices and \code{num_thresholds} is ignored.
    translation: |-
      (facultatif) Si des seuils sont fournis, alors ceux-ci sont utilisés pour
      calculer les matrices de confusion et le paramètre \code{num_thresholds} est ignoré.
  from_logits:
    original: |-
      If \code{TRUE} then we call \code{\link[torch:nnf_softmax]{torch::nnf_softmax()}} in the predictions
      before computing the metric.
    translation: |
       Si \code{TRUE} alors nous appliquons \code{\link[torch:nnf_softmax]{torch::nnf_softmax()}}
       sur les predictions avant le calcul de la métrique.
  average:
    original: |-
      The averaging method:
      \itemize{
      \item \code{'micro'}: Stack all classes and computes the AUROC as if it was a binary
      classification problem.
      \item \code{'macro'}: Finds the AUCROC for each class and computes their mean.
      \item \code{'weighted'}: Finds the AUROC for each class and computes their weighted
      mean pondering by the number of instances for each class.
      \item \code{'none'}: Returns the AUROC for each class in a list.
      }
    translation: |-
      La méthode pour moyenner :
      \itemize{
      \item \code{'micro'} : Empile toutes les classes et calcule l'AUC comme si c'était
      un problème de classification binaire.
      \item \code{'macro'} : Trouve l'AUC pour chaque classe et calcule leur moyenne.
      \item \code{'weighted'} : Trouve l'AUC pour chaque classe et calcule leur 
      moyenne pondéré en fonction du nombre d'instances pour chaque classe.
      \item \code{'none'} : Retourne l'AUC pour chaque classe dans une liste.
      }
description:
  original: |
    The same definition as \href{https://www.tensorflow.org/api_docs/python/tf/keras/metrics/AUC}{Keras}
    is used by default. This is equivalent to the \code{'micro'} method in SciKit Learn
    too. See \href{https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html}{docs}.
  translation: |
    La définition de  \href{https://www.tensorflow.org/api_docs/python/tf/keras/metrics/AUC}{Keras}
    est utilisée par défaut. Cela équivaut à la méthode \code{'micro'} dans SciKit Learn. 
    Voir \href{https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html}{docs}.

details:
  original: |
    \strong{Note} that class imbalance can affect this metric unlike
    the AUC for binary classification.

    Currently the AUC is approximated using the 'interpolation' method described in
    \href{https://www.tensorflow.org/api_docs/python/tf/keras/metrics/AUC}{Keras}.
  translation: |
    \strong{Notez} que le déséquilibre des classes peut affecter cette métrique, 
    contrairement à l'AUC pour une classification binaire.
examples:
  original: |
    if (torch::torch_is_installed()) {
    library(torch)
    actual <- c(1, 1, 1, 0, 0, 0) + 1L
    predicted <- c(0.9, 0.8, 0.4, 0.5, 0.3, 0.2)
    predicted <- cbind(1-predicted, predicted)

    y_true <- torch_tensor(as.integer(actual))
    y_pred <- torch_tensor(predicted)

    m <- luz_metric_multiclass_auroc(thresholds = as.numeric(predicted),
                                     average = "micro")
    m <- m$new()

    m$update(y_pred[1:2,], y_true[1:2])
    m$update(y_pred[3:4,], y_true[3:4])
    m$update(y_pred[5:6,], y_true[5:6])
    m$compute()
    }
  translation: ~
seealso:
  original: "Other luz_metrics: \n\\code{\\link{luz_metric_accuracy}()},\n\\code{\\link{luz_metric_binary_accuracy_with_logits}()},\n\\code{\\link{luz_metric_binary_accuracy}()},\n\\code{\\link{luz_metric_binary_auroc}()},\n\\code{\\link{luz_metric_mae}()},\n\\code{\\link{luz_metric_mse}()},\n\\code{\\link{luz_metric_rmse}()},\n\\code{\\link{luz_metric}()}\n"
  translation: "Autres métrique de luz: \n\\code{\\link{luz_metric_accuracy}()},\n\\code{\\link{luz_metric_binary_accuracy_with_logits}()},\n\\code{\\link{luz_metric_binary_accuracy}()},\n\\code{\\link{luz_metric_binary_auroc}()},\n\\code{\\link{luz_metric_mae}()},\n\\code{\\link{luz_metric_mse}()},\n\\code{\\link{luz_metric_rmse}()},\n\\code{\\link{luz_metric}()}\n"
untranslatable:
- alias
- name
- keyword
- concept
- usage
