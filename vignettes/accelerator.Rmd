---
title: "API des accélérateurs"
output: rmarkdown::html_vignette
lang: fr
vignette: >
  %\VignetteIndexEntry{API des accélérateurs}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(luz)
```

L'API des accélérateurs est une version simplifiée de la [bibliothèque Accelerate](https://github.com/huggingface/accelerate) de Hugging Face.
Elle permet aux utilisateurs d'éviter le code de gestion nécessaire pour écrire les boucles d'apprentissage qui fonctionnent correctement sur plusieurs périphériques de calcul (les CPUs et les GPUs).
Actuellement, elle ne gère que l'utilisation des CPUs et d'un unique GPU.

Cette API est conçue pour être la méthode la plus flexible possible de l'utilisation du package luz. 
Avec l'API des accélérateurs, vous écrivez le code torch d'apprentissage brut et, après quelques modifications, vous gérez automatiquement son deploiement sur les périphériques de calcul du modèle, des optimiseurs et des chargeurs de donnée, donc vous n'avez pas besoin 
d'ajouter partout des `$to(device="cuda")` dans votre code ou penser à l'ordre dans lequel créer le modèle et les optimiseurs.

## Exemple

L'API des accélérateurs est mieux expliquée sur un exemple de différence `diff` du code dans une boucle d'apprentissage torch brut.

```diff
library(torch)
+ library(luz)

+ acc <- accelerator()
- device <- "cpu"

data <- tensor_dataset(
  x = torch_randn(100, 10),
  y = torch_rand(100, 1)
)

dl <- dataloader(data, batch_size = 10)

model <- nn_linear(10, 1)
- model$to(device = device)
opt <- optim_adam(model$parameters)

+ c(model, opt, dl) %<-% acc$prepare(model, opt, dl)

model$train()
coro::loop(for (batch in dl) {

  opt$zero_grad()

-  preds <- model(batch$x$to(device = device))
+  preds <- model(batch$x)
-  loss <- nnf_mse_loss(preds, batch$y$to(device = device))
+  loss <- nnf_mse_loss(preds, batch$y)

  loss$backward()
  opt$step()
})
```

Avec les modifications de code montrées, vous n'avez plus besoin de déplacer manuellement les données et les paramètres entre périphériques, ce qui rend votre code plus facile à lire et moins sujet aux erreurs.

Vous pouvez trouver des informations supplémentaires en utilisant `help(accelerator)`.
