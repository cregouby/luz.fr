---
title: "Get started with luz"
output: rmarkdown::html_vignette
lang: fr
vignette: >
  %\VignetteIndexEntry{Get started with luz}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(luz)
library(torch)
```


Luz est une API de haut niveau pour torch qui vise à encapsuler la boucle d'**entraînement** dans un ensemble de blocs de code réutilisables.
Luz réduit le code de gestion requis pour former un modèle avec torch et évite les erreurs fréquentes sur la séquence des appels à `zero_grad()` - `backward()` - `step()`, et simplifie également le processus de déplacement des données et des modèles entre les CPUs et la GPU.
Luz est conçu pour être très flexible en fournissant une API en couches qui lui permet d'être utile quel que soit le niveau de contrôle dont vous avez besoin pour votre boucle d'entraînement.

Luz est fortement inspiré par d'autres framework de haut-niveau  pour l'apprentissage profond, pour citer quelques-uns:

- [FastAI](https://docs.fast.ai/): nous avons été fortement inspirés par la bibliothèque FastAI, en particulier l'objet `Learner` et l'API callbacks.

- [Keras](https://keras.io/): Nous sommes également fortement inspirés par Keras, en particulier les noms de callback. L'interface du module `lightning` est aussi similaire à `compile`.

- [PyTorch Lightning](https://lightning.ai/pages/open-source/): L'idée que le `luz_module` soit une sous-classe de `nn_module` s'inspire de l'objet **`LightningModule`** dans la `lightning`.

- [HuggingFace Accelerate](https://huggingface.co/docs/accelerate/): L'API interne de placement sur les périphériques de calcul est fortement inspirée par Accelerate, mais est beaucoup plus modeste dans les fonctionnalités.

## Entraînement d'un `nn_module`

Luz tente, autant que possible, de réutiliser les structures existantes de torch.
Un modèle en luz est défini de la même manière que vous le définiriez si vous utilisez torch brut.
Par exemple, voici la définition d'un CNN feed-forward qui peut être utilisé pour classer les chiffres de l'ensemble de données MNIST:
```{r, eval = FALSE}
net <- nn_module(
  "Net",
  initialize = function(num_class) {
    self$conv1 <- nn_conv2d(1, 32, 3, 1)
    self$conv2 <- nn_conv2d(32, 64, 3, 1)
    self$dropout1 <- nn_dropout2d(0.25)
    self$dropout2 <- nn_dropout2d(0.5)
    self$fc1 <- nn_linear(9216, 128)
    self$fc2 <- nn_linear(128, num_class)
  },
  forward = function(x) {
    x <- self$conv1(x)
    x <- nnf_relu(x)
    x <- self$conv2(x)
    x <- nnf_relu(x)
    x <- nnf_max_pool2d(x, 2)
    x <- self$dropout1(x)
    x <- torch_flatten(x, start_dim = 2)
    x <- self$fc1(x)
    x <- nnf_relu(x)
    x <- self$dropout2(x)
    x <- self$fc2(x)
    x
  }
)
```


Nous pouvons maintenant entraîner ce modèle dans le `train_dl` et l'évaluer dans le `torch::dataloaders()` nomé `test_dl` avec:
```{r, eval = FALSE}
fitted <- net %>%
  setup(
    loss = nn_cross_entropy_loss(),
    optimizer = optim_adam,
    metrics = list(
      luz_metric_accuracy
    )
  ) %>%
  set_hparams(num_class = 10) %>% 
  set_opt_hparams(lr = 0.003) %>% 
  fit(train_dl, epochs = 10, valid_data = test_dl)
```


Voyons en détail ce qui se passe dans ce bloc de code :

1. La fonction `setup` vous permet de configurer la fonction de coût (objectif) et l'optimiseur que vous utiliserez pour former votre modèle. En option, vous pouvez passer une liste de métriques qui sont suivies pendant la procédure d'entraînement. **Note:** la fonction de coût peut être n'importe quelle fonction prenant en entrée `input` et `target` et renvoyant une valeur de tenseur scalaire, et l'optimiseur peut être n'importe quel optimisation de torch de base ou personnalisé créé avec la fonction `torch::optimizer()`.
2. La fonction `set_hparams()` vous permet de définir des hyperparamètres qui seront passés à la méthode `initialise()` du module. Par exemple, dans ce cas, nous passons `num_classes = 10`.
3. La fonction `set_opt_hparams()` vous permet de passer des hyperparamètres utilisés par la fonction d'optimisation. Par exemple, `optim_adam()` peut prendre le paramètre `lr` spécifiant le taux d'apprentissage et nous le spécifions avec `lr = 0.003`.
4. La méthode `fit` prendra les spécifications du modèle fournies par `setup()` et exécutera la procédure d'entraînement en utilisant les jeux de donnée d'entraînement et de validation spécifiés dans des `torch::dataloaders()` ainsi que le nombre d'époques. **Note:** nous réutiliserons à nouveau les structures de données de base de torch, au lieu de fournir notre propre fonctionnalité de chargement de données.
5. L ' objet retourné `fitted` contient le modèle entraîné ainsi que le relevé des métriques et des pertes produites au cours de l'entraînement; il peut également servir à établir des predictions et à évaluer le modèle entraîné sur d'autres jeux de données.

Au moment de l'installation, luz utilisera l'accélérateur le plus rapide possible; si un GPU compatible CUDA est disponible, il sera utilisé, sinon le calcul sera affecté à la CPU.
Il déplace également automatiquement les données, les optimisateurs et les modèles vers le périphérique sélectionné afin que vous n'ayez pas besoin de les manipuler manuellement (qui constitue une grande source d'erreurs en général).

Pour créer des prédictions à partir du modèle entraîné, vous pouvez utiliser la méthode `predict`:

```{r, eval = FALSE}
predictions <- predict(fitted, test_dl)
```


## La boucle d'entraînement

Vous avez maintenant une idée générale de la façon d'utiliser la fonction `fit`. Maintenant il est important d'avoir un aperçu de ce qui se passe à l'intérieur.
Voici ce que `fit` exécute (en pseudocode).
Ce n'est pas détaillé complètement, mais celà devrait vous aider à construire votre intuition:

```{r, eval = FALSE}
# -> Initialize objects: model, optimizers.
# -> Select fitting device.
# -> Move data, model, optimizers to the selected device.
# -> Start training
for (epoch in 1:epochs) {
  # -> Training procedure
  for (batch in train_dl) {
    # -> Calculate model `forward` method.
    # -> Calculate the loss
    # -> Update weights
    # -> Update metrics and tracking loss
  }
  # -> Validation procedure
  for (batch in valid_dl) {
    # -> Calculate model `forward` method.
    # -> Calculate the loss
    # -> Update metrics and tracking loss
  }
}
# -> End training
```


Les métriques

L'une des parties les plus importantes des projets d'apprentissage automatique est le choix des métriques.
Luz permet de suivre de nombreuses métriques différentes pendant l'entraînement avec des changements de code minimes.

Pour suivre les métriques, il suffit de modifier le paramètre `metrics` dans la fonction `setup`:

```{r, eval=FALSE}
fitted <- net %>%
  setup(
    ...
    metrics = list(
      luz_metric_accuracy
    )
  ) %>%
  fit(...)
```


Luz fournit des implémentations de quelques-unes des métriques les plus utilisées.
Si une métrique n'est pas disponible, vous pouvez toujours en implémenter une nouvelle en utilisant la fonction `luz_metric`.

```{r, child='../man/rmd/metrics.Rmd'}
```

## Évaluer

```{r, child='../man/rmd/evaluate.Rmd'}
```


Personnalisation avec les callbacks

Luz fournit différentes façons de personnaliser la boucle d'entraînement, en fonction du niveau de contrôle dont vous avez besoin pendant qu'elle s'exécute.
Le moyen le plus rapide et le plus « réutilisable » est via les **callbacks**. Ils permettent de créer des modifications d'entraînement qui peuvent être utilisées dans de nombreuses situations.

La boucle d'entraînement en luz a de nombreux *points d'arrêt* qui peuvent appeler des fonctions R arbitraires.
Cette fonctionnalité vous permet de personnaliser le processus de formation sans avoir à modifier la logique générale de formation.

Luz implémente 3 callbacks par défaut qui se produisent dans chaque procédure de formation :

- ** callback train-eval**: Définit le modèle à "train()" ou "eval()" selon que la procédure est en cours de  d'entraînement ou de validation.

- ** callback mesurable** : évaluer les paramètres au cours du processus d'entraînement et de validation.

- ** callback sur les progrès** : implémente une barre de progression et imprime les informations sur les progrès au cours de l'entraînement.

Vous pouvez également implémenter des callbacks personnalisés qui modifient ou agissent spécifiquement pour votre procédure d'entraînement.
Par exemple:
```{r}
#| child: ../man/rmd/callbacks.Rmd


```

```{r}
#| child: ../man/rmd/ctx.Rmd


```



Les attributs de `ctx` peuvent être utilisés pour produire le comportement souhaité des callbacks.
Vous pouvez trouver des informations sur l'objet contextuel en utilisant `help("ctx")`.
Dans notre exemple, nous utilisons l'attribut `ctx$iter` pour imprimer le numéro d'itération pour chaque lot d'entraînement.

Prochaines étapes

Dans cet article, vous avez appris à entraîner votre premier modèle en utilisant luz et les bases de la personnalisation en utilisant à la fois des métriques personnalisées et des callbacks.

Luz permet également des modifications plus souples de la boucle d'entraînement décrite dans la rubrique "vignette("custom-loop")".

Vous devriez maintenant être en mesure de suivre les exemples marqués avec la catégorie de base dans la [ galerie d'exemples] (https://mlverse.github.io/luz/articles/examples/index.html).
